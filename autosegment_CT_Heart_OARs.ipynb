{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cerr/pyCERR-Notebooks/blob/main/autosegment_CT_Heart_OARs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNoydh-RsgcD"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will demonstrate how to apply a pre-trained AI model to segment the Heart sub-structures on a lung CT scan using pyCERR.\n",
    "\n",
    "## AI model\n",
    "* The segmentation model was trained and validated on CT scans used for RT planning. It does not work optimally on diagnostic CTs or scans in positions other than Head First Supine.\n",
    "* The trained model is distributed along with python libraries and other dependencies via a conda package.\n",
    "* The model requires acess to a GPU.\n",
    "\n",
    "### Running the model\n",
    "\n",
    "Update locations of input data and model directorues in section 2 of this notebook.\n",
    "* Conda packge is location: condaEnvDir\n",
    "* Inference script location: wrapperPath\n",
    "* Inference script args\n",
    "```python\n",
    "!python {wrapperPath} {input_nii_directory} {output_nii_directory}\n",
    "```\n",
    "\n",
    "### License\n",
    "By downloading the software you are agreeing to the following terms and conditions as well as to the Terms of Use of CERR software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\" AND CERR DEVELOPMENT TEAM AND ITS COLLABORATORS DO NOT MAKE ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, NOR DO THEY ASSUME ANY LIABILITY OR RESPONSIBILITY FOR THE USE OF THIS SOFTWARE.\n",
    "    \n",
    "This software is for research purposes only and has not been approved for clinical use.\n",
    "\n",
    "Software has not been reviewed or approved by the Food and Drug Administration, and is for non-clinical, IRB-approved Research Use Only. In no event shall data or images generated through the use of the Software be used in the provision of patient care.\n",
    "\n",
    "You may publish papers and books using results produced using software provided that you reference the appropriate citations:\n",
    "*  Heart sub-structures model: https://doi.org/10.1016/j.phro.2020.05.009\n",
    "*  CERR library of model implementations: https://doi.org/10.1016/j.ejmp.2020.04.011\n",
    "*  CERR software: https://doi.org/10.1118/1.1568978\n",
    "*  CERR radiomics: https://doi.org/10.1002/mp.13046\n",
    "\n",
    "\n",
    "YOU MAY NOT DISTRIBUTE COPIES of this software, or copies of software derived from this software, to others outside your organization without specific prior written permission from the CERR development team except where noted for specific software products.\n",
    "\n",
    "All Technology and technical data delivered under this Agreement are subject to US export control laws and may be subject to export or import regulations in other countries. You agree to comply strictly with all such laws and regulations and acknowledge that you have the responsibility to obtain such licenses to export, re-export, or import as may be required after delivery to you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKyePuucYlbA"
   },
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo2uAuSxYlbB"
   },
   "source": [
    "## Download planning CT DICOM from ***dataUrl*** to ***dataDownloadDir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B-XrWTGa4sG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "workspaceDir = r'/home/jupyter' # r'/content' for Colab\n",
    "dataUrl='http://path.to/data'\n",
    "dataDownloadDir = os.path.join(workspaceDir,'sampleData','testPlanningCTdicom')\n",
    "os.makedirs(dataDownloadDir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3ucdyu0YlbB"
   },
   "outputs": [],
   "source": [
    "! wget -O sampleData.gz -L {dataUrl}\n",
    "! tar xf sampleData.gz -C {dataDownloadDir}\n",
    "! rm sampleData.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSpNk-BrYlbD"
   },
   "source": [
    "##  Define paths for input DICOM, output segmentation and session directories\n",
    "\n",
    "Specify paths to the DICOM input data, desired output directory, and temporary (session) directory used to store intermediate results. The input data is structures such that DICOM per patient scan is in an individual directory.\n",
    "\n",
    "&nbsp;&nbsp;Input Directory  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Pat1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img1.dcm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img2.dcm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;....  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;....  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Pat2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img1.dcm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img2.dcm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;....  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;....  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1PBqiDCYlbD"
   },
   "outputs": [],
   "source": [
    "#Paths to input data and conda env with pre-trained models\n",
    "#inputDicomPath = os.path.join(dataDownloadDir,'rtog-0617','0617_test')  # Replace with apropriate path to your dataset\n",
    "inputDicomPath = os.path.join(dataDownloadDir,'0617_test')\n",
    "outputDicomPath = os.path.join(workspaceDir, 'AIoutput')\n",
    "sessionPath = os.path.join(workspaceDir, 'temp')\n",
    "\n",
    "if not os.path.exists(outputDicomPath):\n",
    "  os.mkdir(outputDicomPath)\n",
    "\n",
    "if not os.path.exists(sessionPath):\n",
    "  os.mkdir(sessionPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnoZxbKnsgcG"
   },
   "source": [
    "## Download the pre-packaged Anaconda environment to *heartCondaEnvDir*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUc1tZzbsgcK"
   },
   "outputs": [],
   "source": [
    "# Location of LungOAR conda environment\n",
    "heartCondaEnvDir = os.path.join(workspaceDir, 'pretrainedHeartModel')\n",
    "\n",
    "# Download packaged environment for the Heart segmentation model\n",
    "boxHash = 'H4sIAAAAAAAAAwXBUQqAIAwA0P8O4xAKotvoCjWzqVuZnr73vEjmDSBxRFSWPoWUgL2pxw4sRgLCfAXNNMyS8W6llxaf16WziV11lErKjekHlA19CEgAAAA='\n",
    "saveFileName = 'ct_heart_oar.tar.gz'\n",
    "\n",
    "!mkdir -p {heartCondaEnvDir}\n",
    "!wget `base64 -d <<<{boxHash} | gunzip` -O {saveFileName}\n",
    "!tar xf {saveFileName} -C {heartCondaEnvDir}\n",
    "!rm {saveFileName}\n",
    "\n",
    "# Path to conda environment activate script\n",
    "heartEnvActivateScript = os.path.join(heartCondaEnvDir, 'bin', 'activate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cExoky2sYlbE"
   },
   "source": [
    "## Download the inference script and model weights to *wrapperInstallDir*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0G_2-MkYlbF"
   },
   "outputs": [],
   "source": [
    "wrapperInstallDir = os.path.join(workspaceDir, 'CT_cardiac_structures_deeplab')\n",
    "!git clone https://github.com/cerr/CT_cardiac_structures_deeplab.git {wrapperInstallDir}\n",
    "wrapperPath = os.path.join(wrapperInstallDir, 'model_wrapper','runSegmentation.py')\n",
    "modelWeightZipPath = os.path.join(wrapperInstallDir,'model.gz')\n",
    "modelWeightPath = os.path.join(wrapperInstallDir,'model')\n",
    "modelHash = 'H4sIAAAAAAAAAwXBUQqAIBAFwP8O46YlZLexLTVELN9S0umbSSIXVqKCzKy22hXXQki+HTtBvJxM1Zq5I1o4fY+hvdnrMTwIi5mcRYwqfsMPzAV1z0gAAAA='\n",
    "!wget -O {modelWeightZipPath} -L `base64 -d <<<{modelHash} | gunzip`\n",
    "!tar xf {modelWeightZipPath} -C {wrapperInstallDir}\n",
    "!rm {modelWeightZipPath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsKXYI-dsgcK"
   },
   "source": [
    "# Install pyCERR\n",
    "\n",
    "pyCERR is used for pre and post-processing of DICOM as required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRGzE5J7sgcL"
   },
   "outputs": [],
   "source": [
    "! pip install \"pyCERR[napari] @ git+https://github.com/cerr/pyCERR.git@testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfcIIiLQsgcM"
   },
   "source": [
    "# Functions for data pre- and post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m955pAJsgcN"
   },
   "source": [
    "## Crop scan to Lung extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAXBqkTasgcN"
   },
   "outputs": [],
   "source": [
    "from cerr.dataclasses import structure as cerrStr\n",
    "from cerr.contour import rasterseg as rs\n",
    "from cerr.utils import identifyScan, imageProc\n",
    "from cerr.utils import bbox\n",
    "import numpy as np\n",
    "\n",
    "def processInputData(scanNum, planC, lungNameList=['Lung_total', 'Lung_L', 'Lung_R']):\n",
    "\n",
    "    if isinstance(lungNameList, str):\n",
    "        lungNameList = [lungNameList]\n",
    "\n",
    "    # Extract scanArray\n",
    "    scan3M = planC.scan[scanNum].getScanArray()\n",
    "    mask3M = np.zeros(scan3M.shape, dtype=bool)\n",
    "\n",
    "    # List of Structure names\n",
    "    strNames = [s.structureName for s in planC.structure]\n",
    "    numOrigStructs = len(strNames)\n",
    "\n",
    "    # Get total lung mask\n",
    "    for lungName in lungNameList:\n",
    "        lungInd = cerrStr.getMatchingIndex(lungName, strNames, 'exact')\n",
    "        if len(lungInd) > 0:\n",
    "            # Get lung extents\n",
    "            mask3M = mask3M | rs.getStrMask(lungInd[0], planC)\n",
    "\n",
    "    if not np.any(mask3M):\n",
    "        raise Exception('Lung contour name did not match any structures in planC')\n",
    "\n",
    "    # Create cropped scan\n",
    "    rmin,rmax,cmin,cmax,smin,smax,_ = bbox.compute_boundingbox(mask3M)\n",
    "    x,y,z = planC.scan[0].getScanXYZVals()\n",
    "    xCropV = x[cmin:cmax]\n",
    "    yCropV = y[rmin:rmax]\n",
    "    zCropV = z[smin:smax]\n",
    "    scan3M = planC.scan[0].getScanArray()\n",
    "    scanCrop3M = scan3M[rmin:rmax,cmin:cmax,smin:smax]\n",
    "\n",
    "    return scanCrop3M, (xCropV, yCropV, zCropV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1KHjP1LsgcO"
   },
   "source": [
    "## Import AI segmentations to planC and retain only the largest connected component for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_GjRVWtsgcP"
   },
   "outputs": [],
   "source": [
    "#Import label map to CERR\n",
    "import glob\n",
    "from cerr import plan_container as pc\n",
    "\n",
    "atriaLabelDict = {1: 'DL_Atria'}\n",
    "heartSubSegDict = {2: 'DL_AORTA', 3: 'DL_LA',\n",
    "                   4: 'DL_LV', 5: 'DL_RA',\n",
    "                   6: 'DL_RV', 7: 'DL_IVC',\n",
    "                   8: 'DL_SVC', 9: 'DL_PA'}\n",
    "heartSegDict = {1: 'DL_heart'}\n",
    "periLabelDict = {1: 'DL_Pericardium'}\n",
    "ventriLabelDict = {1: 'DL_Ventricles'}\n",
    "\n",
    "def postProcAndImportSeg(outputDir,procScanNum,scanNum,planC):\n",
    "    niiGlob = glob.glob(os.path.join(outputDir,'*.nii.gz'))\n",
    "    for segFile in niiGlob:\n",
    "        print('Importing ' + segFile + '...')\n",
    "        # Get segFile name\n",
    "        if 'heart.nii.gz' in segFile:\n",
    "            strToLabelMap = heartSubSegDict\n",
    "        elif 'heartStructure.nii.gz' in segFile:\n",
    "            strToLabelMap = heartSegDict\n",
    "        elif 'atria.nii.gz' in segFile:\n",
    "            strToLabelMap = atriaLabelDict\n",
    "        elif 'pericardium.nii.gz' in segFile:\n",
    "            strToLabelMap = periLabelDict\n",
    "        elif 'ventricles.nii.gz' in segFile:\n",
    "            strToLabelMap = ventriLabelDict\n",
    "        numLabel = len(strToLabelMap)\n",
    "        numStrOrig = len(planC.structure)\n",
    "        planC = pc.load_nii_structure(segFile, procScanNum, planC, \\\n",
    "                                  labels_dict = strToLabelMap)\n",
    "        numStructs = len(planC.structure)\n",
    "        cpyStrNumV = np.arange(numStrOrig,numStructs)\n",
    "        numComponents = 1\n",
    "        for label in range(numLabel):\n",
    "            # Copy segmentaton from cropped to the original scan\n",
    "            planC = cerrStr.copyToScan(cpyStrNumV[label], scanNum, planC)\n",
    "            origStrNum = numStructs + label -1\n",
    "            mask3M = rs.getStrMask(origStrNum,planC)\n",
    "            # Post-process segmentation to retain the largest connected component\n",
    "            procMask3M, _ = imageProc.getLargestConnComps(mask3M,numComponents)\n",
    "            strName = planC.structure[origStrNum].structureName #strToLabelMap[label+1]\n",
    "            planC = pc.import_structure_mask(procMask3M, scanNum, strName, [], planC)\n",
    "            # Delete segmentation before post-processing\n",
    "            del planC.structure[origStrNum]\n",
    "\n",
    "    return planC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj7t-J4isgcQ"
   },
   "source": [
    "# Segment OARs for all the CT scans located at *inputDicomPath*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCuUi5D6sgcS"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import cerr\n",
    "from cerr import plan_container as pc\n",
    "from cerr.dataclasses import scan as cerrScn\n",
    "from cerr.ai import createSessionDir as cdir\n",
    "from cerr.dcm_export import rtstruct_iod\n",
    "\n",
    "# Loop over pyCERR files\n",
    "fileList = os.listdir(inputDicomPath)\n",
    "numFiles = len(fileList)\n",
    "modality = 'CT SCAN'\n",
    "lungNameList = ['LUNG_TOTAL', 'LUNG_CNTR', 'LUNG_IPSI']\n",
    "\n",
    "for iFile in range(numFiles):\n",
    "\n",
    "    dcmDir = os.path.join(inputDicomPath,fileList[iFile])\n",
    "\n",
    "    # Create session dir to store temporary data\n",
    "    modInputPath, modOutputPath = cdir.createSessionDir(sessionPath, inputDicomPath)\n",
    "\n",
    "    # Import DICOM to planC\n",
    "    planC = pc.load_dcm_dir(dcmDir)\n",
    "\n",
    "    # Identify scan index in  planC\n",
    "    scanIdS = {\"imageType\": modality}\n",
    "    matchScanV = identifyScan.getScanNumForIdentifier(scanIdS, planC, False)\n",
    "    scanNum = matchScanV[0]\n",
    "\n",
    "    # Pre-process data\n",
    "    procScan3M, resizeGridS = processInputData(scanNum, planC, lungNameList)\n",
    "    planC = pc.import_scan_array(procScan3M, resizeGridS[0], \\\n",
    "            resizeGridS[1], resizeGridS[2], modality, scanNum, planC)\n",
    "    procScanNum = len(planC.scan) - 1\n",
    "\n",
    "    # Export inputs to NIfTI\n",
    "    scanFilename = os.path.join(modInputPath, f\"{fileList[iFile]}_scan_3D.nii.gz\")\n",
    "    planC.scan[procScanNum].save_nii(scanFilename)\n",
    "\n",
    "    numOrigStructs = len(planC.structure)\n",
    "\n",
    "    # Apply model\n",
    "    runScript = \"source \" + heartEnvActivateScript + \" && python \" + wrapperPath \\\n",
    "                  + \" \" + modInputPath + \" \" + modOutputPath\n",
    "    print(runScript)\n",
    "    subprocess.run(runScript,\n",
    "                 capture_output=False,\n",
    "                  shell=True,\n",
    "                  executable=\"/bin/bash\")\n",
    "\n",
    "    # Import results to planC\n",
    "    planC = postProcAndImportSeg(modOutputPath,procScanNum,scanNum,planC)\n",
    "\n",
    "    numStructs = len(planC.structure)\n",
    "\n",
    "    # Export segmentations to DICOM\n",
    "    structFileName = fileList[iFile] + '_AI_seg.dcm'\n",
    "    structFilePath = os.path.join(outputDicomPath,structFileName)\n",
    "    structNumV = np.arange(numOrigStructs, numStructs)\n",
    "    indOrigV = np.array([cerrScn.getScanNumFromUID(planC.structure[structNum].assocScanUID, planC) for structNum in structNumV], dtype=int)\n",
    "    origIndsToExportV = structNumV[indOrigV == scanNum]\n",
    "    seriesDescription = \"AI Generated\"\n",
    "    exportOpts = {'seriesDescription': seriesDescription}\n",
    "    rtstruct_iod.create(origIndsToExportV,structFilePath,planC,exportOpts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional - Download the outputDicomPath to Workspace bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workspaceBucket = os.environ['WORKSPACE_BUCKET']\n",
    "# !gcloud storage cp -r {outputDicomPath} {workspaceBucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLzFp-l_sgcT"
   },
   "source": [
    "# Display results for the last CT scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8FTwNqhsgcT"
   },
   "source": [
    "## Display using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerr.viewer import showMplNb\n",
    "\n",
    "showMplNb(scanNum, origIndsToExportV, planC,\\\n",
    "          windowCenter=-400, windowWidth=2000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
